{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ensemble Model**: `Similar Type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate bagging, rf, and boosting ensemble. First, set the python library that will be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a synthetic binary classification problem with 1000 examples and 10 input features using make_classification()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 10), (200, 10))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y = make_classification(n_samples=1000, n_features=10, random_state=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =.2,random_state=2020,stratify=y)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Bagging Ensembles**\n",
    "\n",
    "**Definition**: also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance within a noisy data set. In bagging, a random sample of data in a training set is selected with replacement—meaning that the individual data points can be chosen more than once. After generating several data samples, these weak models are then trained independently. Depending on the type of task—regression or classification, for example—the average or majority of those predictions yield a more accurate estimate. \n",
    "\n",
    "Bagging algorithm has three basic steps:\n",
    "\n",
    "- **Bootstrapping**:  Bagging leverages a bootstrapping sampling technique to create diverse samples. This resampling method generates different subsets of the training data set by selecting data points at random and with replacement. This means that each time you select a data point from the training data set, you are able to select the same instance multiple times. As a result, a value or instance repeated twice (or more) in a sample.\n",
    "- **Parallel training**: These bootstrap samples are then trained independently and in parallel with each other using weak or base learners.\n",
    "- **Aggregation**: Finally, depending on the task (that is, regression or classification), an average or a majority of the predictions are taken to compute a more accurate estimate. In the case of regression, an average is taken of all the outputs predicted by the individual classifiers; this is known as soft voting. For classification problems, the class with the highest majority of votes is accepted; this is known as hard voting or majority voting.\n",
    "\n",
    "The key challenges of bagging include:\n",
    "\n",
    "- **Loss of interpretability**: It’s difficult to draw very precise business insights through bagging because due to the averaging involved across predictions. \n",
    "- **Computationally expensive**: Bagging slows down and grows more intensive as the number of iterations increase. \n",
    "- **Less flexible**: As a technique, bagging works particularly well with algorithms that are less stable. One that are more stable or subject to high amounts of bias do not provide as much benefit as there’s less variation within the data set of the model. \n",
    "\n",
    "**Practice:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "clf = BaggingClassifier(\n",
    "    estimator=logreg,\n",
    "    max_samples=.7,\n",
    "    n_estimators=50,\n",
    ")\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "score = cross_val_score(clf,X_train,y_train, scoring='accuracy',cv=skfold)\n",
    "print(f'Accuracy : {score.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest has multiple decision trees as base learning models, several random trees make a Random Forest. The Random Forest model uses bagging, where we randomly perform row sampling and feature sampling from the dataset froming datasets for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    criterion='entropy',\n",
    "    max_features='sqrt',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "score = cross_val_score(clf,X_train,y_train, scoring='accuracy',cv=skfold)\n",
    "print(f'Accuracy : {score.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Boosting Ensembles**\n",
    "\n",
    "**Definition**: is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added. \n",
    "\n",
    "The steps to build and combine these models are as:\n",
    "\n",
    "- Initialize the weights\n",
    "- Train weak classifiers\n",
    "- Calculate the error rate and importance of each weak model\n",
    "- Update data point weight for each data point Wi \n",
    "- Normalize the Instance weight \n",
    "- Repeat steps 2-5 for K iterations \n",
    "\n",
    "**Practice:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost, short for Adaptive Boosting, is an ensemble machine learning algorithm that can be used in a wide variety of classification and regression tasks. It is a supervised learning algorithm that is used to classify data by combining multiple weak or base learners (e.g., decision trees) into a strong learner. AdaBoost works by weighting the instances in the training dataset based on the accuracy of previous classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(\n",
    "    algorithm='SAMME',\n",
    "    n_estimators=50,\n",
    "    learning_rate=1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(clf,X_train,y_train, scoring='accuracy',cv=skfold)\n",
    "print(f'Accuracy : {score.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is one of the variants of ensemble methods where you create multiple weak models and combine them to get better performance as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=.1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(clf,X_train,y_train, scoring='accuracy',cv=skfold)\n",
    "print(f'Accuracy : {score.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a more regularized form of Gradient Boosting. XGBoost uses advanced regularization (L1 & L2), which improves model generalization capabilities. XGBoost delivers high performance as compared to Gradient Boosting. Its training is very fast and can be parallelized across clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.89\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=.1,\n",
    "    random_state=0,\n",
    "    subsample=.7\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = cross_val_score(clf,X_train,y_train, scoring='accuracy',cv=skfold)\n",
    "print(f'Accuracy : {score.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Application`\n",
    "\n",
    "`Load Dataset`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.045</td>\n",
       "      <td>37.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.54</td>\n",
       "      <td>10.4</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.35</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.040</td>\n",
       "      <td>60.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>5.8</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.44</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.029</td>\n",
       "      <td>5.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.43</td>\n",
       "      <td>11.7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.38</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.058</td>\n",
       "      <td>55.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.032</td>\n",
       "      <td>35.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>12.5</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "422            7.0              0.21         0.28             8.6      0.045   \n",
       "107            7.1              0.23         0.35            16.5      0.040   \n",
       "253            5.8              0.24         0.44             3.5      0.029   \n",
       "235            7.2              0.23         0.38            14.3      0.058   \n",
       "311            5.0              0.55         0.14             8.3      0.032   \n",
       "\n",
       "     free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "422                 37.0                 221.0   0.9954  3.25       0.54   \n",
       "107                 60.0                 171.0   0.9990  3.16       0.59   \n",
       "253                  5.0                 109.0   0.9913  3.53       0.43   \n",
       "235                 55.0                 194.0   0.9979  3.09       0.44   \n",
       "311                 35.0                 164.0   0.9968  3.53       0.51   \n",
       "\n",
       "     alcohol  quality  \n",
       "422     10.4      6.0  \n",
       "107      9.1      6.0  \n",
       "253     11.7      3.0  \n",
       "235      9.0      6.0  \n",
       "311     12.5      8.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../csv/white_wine.csv')\n",
    "data.sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Data Cleaning`\n",
    "\n",
    "**Duplicated Value**, Deteksi dan kuantifikasi duplikasi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data duplikat: 84\n",
      "Persentase data duplikat: 16.15%\n"
     ]
    }
   ],
   "source": [
    "print(f'Jumlah data duplikat: {data.duplicated().sum()}')\n",
    "print(f'Persentase data duplikat: {data.duplicated().sum()/len(data)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebesar 16.15% data terindikasi duplikat, maka hilangkan salah satunya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling duplikasi data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(keep='first', inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Value**, Deteksi dan kuantifikasi missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      1\n",
       "sulphates               1\n",
       "alcohol                 1\n",
       "quality                 1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat missing value pada kolom ph, sulphates, alcohol dan quality. Kita akan drop saja missing value tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change Target**, Mengubah target menjadi kategorikal biner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>30.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            6.2              0.32         0.16             7.0      0.045   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 30.0                 136.0   0.9949  3.18       0.47   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        0  \n",
       "1      9.5        0  \n",
       "2     10.1        0  \n",
       "3      9.9        0  \n",
       "4      9.6        0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['quality'] = np.where(data['quality']> 6, 1, 0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Data Splitting`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagi dataset menjadi train dan test set dengan komposisi 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = data.drop(columns=['quality'])\n",
    "y = data['quality']\n",
    "\n",
    "# Separate data into train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(435, 12)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, 11)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Model Experiment`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameter dasar untuk menginisialisasi experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "knn = KNeighborsClassifier()\n",
    "dtree = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "\n",
    "# Base Models\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('scaling', scaler),\n",
    "    ('modeling', logreg)\n",
    "])\n",
    "\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaling', scaler),\n",
    "    ('modeling', knn)\n",
    "])\n",
    "\n",
    "dtree_pipeline = Pipeline([\n",
    "    ('modeling', dtree)\n",
    "])\n",
    "\n",
    "# Meta learner\n",
    "meta_logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Voting Classifier (Hard)\n",
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('clf1', logreg_pipeline),\n",
    "        ('clf2', knn_pipeline),\n",
    "        ('clf3', dtree_pipeline)\n",
    "    ], voting='hard'\n",
    ")\n",
    "\n",
    "# Voting Classifier (Soft)\n",
    "voting_clf_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('clf1', logreg_pipeline),\n",
    "        ('clf2', knn_pipeline),\n",
    "        ('clf3', dtree_pipeline)\n",
    "    ], voting='soft'\n",
    ")\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('clf1', logreg_pipeline),\n",
    "        ('clf2', knn_pipeline),\n",
    "        ('clf3', dtree_pipeline)\n",
    "    ],\n",
    "    final_estimator=meta_logreg\n",
    ")\n",
    "\n",
    "# Bagging\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=logreg,\n",
    "    n_estimators=50,\n",
    "    max_samples=.7,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# RF\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    criterion='entropy',\n",
    "    max_features='sqrt',\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "ab_clf = AdaBoostClassifier(\n",
    "    algorithm='SAMME',\n",
    "    n_estimators=50,\n",
    "    learning_rate=1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=.1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=.1,\n",
    "    random_state=0,\n",
    "    subsample=.7\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define models\n",
    "models = [logreg_pipeline, knn_pipeline, dtree_pipeline, \n",
    "          voting_clf_hard, voting_clf_soft, stacking_clf,bagging_clf,rf_clf,ab_clf,gb_clf,xgb_clf]\n",
    "model_names = ['LogisticRegression', 'KNeigborsClassifier', 'DecisionTree',\n",
    "               'Voting Classifier (Hard)', 'Voting Classifier (Soft)', 'Stacking Classifier',\n",
    "               'BaggingClassifier','RandomForestClassifier','AdaBoostClassifier','GradientBoostingClassifier','XGBClassifier']\n",
    "\n",
    "# Create list to store evaluation score\n",
    "f1_mean = []\n",
    "f1_std = []\n",
    "all_f1 = []\n",
    "\n",
    "# Cross Validation\n",
    "for model in models:\n",
    "\n",
    "    skfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    model_cv = cross_val_score(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=skfold,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    f1_mean.append(model_cv.mean())    \n",
    "    f1_std.append(model_cv.std())    \n",
    "    all_f1.append(model_cv.round(4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mean f1</th>\n",
       "      <th>std f1</th>\n",
       "      <th>all score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>[1.0, 0.96, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stacking Classifier</td>\n",
       "      <td>0.975304</td>\n",
       "      <td>0.020204</td>\n",
       "      <td>[1.0, 0.96, 0.96, 1.0, 0.9565]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.941275</td>\n",
       "      <td>0.056419</td>\n",
       "      <td>[0.96, 0.8333, 1.0, 0.9565, 0.9565]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.939036</td>\n",
       "      <td>0.045133</td>\n",
       "      <td>[0.96, 0.8696, 1.0, 0.9091, 0.9565]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.932382</td>\n",
       "      <td>0.057347</td>\n",
       "      <td>[1.0, 0.8333, 0.963, 0.9091, 0.9565]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Voting Classifier (Hard)</td>\n",
       "      <td>0.923789</td>\n",
       "      <td>0.049178</td>\n",
       "      <td>[0.96, 0.8333, 0.96, 0.9565, 0.9091]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Voting Classifier (Soft)</td>\n",
       "      <td>0.902577</td>\n",
       "      <td>0.089754</td>\n",
       "      <td>[0.96, 0.7273, 0.96, 0.9565, 0.9091]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.888970</td>\n",
       "      <td>0.116277</td>\n",
       "      <td>[0.96, 0.6667, 1.0, 0.9091, 0.9091]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.859130</td>\n",
       "      <td>0.101001</td>\n",
       "      <td>[0.96, 0.6957, 0.96, 0.88, 0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeigborsClassifier</td>\n",
       "      <td>0.806192</td>\n",
       "      <td>0.094593</td>\n",
       "      <td>[0.8333, 0.6364, 0.7826, 0.9091, 0.8696]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>0.502190</td>\n",
       "      <td>0.156940</td>\n",
       "      <td>[0.5455, 0.2222, 0.6957, 0.5714, 0.4762]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model   mean f1    std f1  \\\n",
       "0           LogisticRegression  0.992000  0.016000   \n",
       "5          Stacking Classifier  0.975304  0.020204   \n",
       "9   GradientBoostingClassifier  0.941275  0.056419   \n",
       "8           AdaBoostClassifier  0.939036  0.045133   \n",
       "10               XGBClassifier  0.932382  0.057347   \n",
       "3     Voting Classifier (Hard)  0.923789  0.049178   \n",
       "4     Voting Classifier (Soft)  0.902577  0.089754   \n",
       "7       RandomForestClassifier  0.888970  0.116277   \n",
       "2                 DecisionTree  0.859130  0.101001   \n",
       "1          KNeigborsClassifier  0.806192  0.094593   \n",
       "6            BaggingClassifier  0.502190  0.156940   \n",
       "\n",
       "                                   all score  \n",
       "0                 [1.0, 0.96, 1.0, 1.0, 1.0]  \n",
       "5             [1.0, 0.96, 0.96, 1.0, 0.9565]  \n",
       "9        [0.96, 0.8333, 1.0, 0.9565, 0.9565]  \n",
       "8        [0.96, 0.8696, 1.0, 0.9091, 0.9565]  \n",
       "10      [1.0, 0.8333, 0.963, 0.9091, 0.9565]  \n",
       "3       [0.96, 0.8333, 0.96, 0.9565, 0.9091]  \n",
       "4       [0.96, 0.7273, 0.96, 0.9565, 0.9091]  \n",
       "7        [0.96, 0.6667, 1.0, 0.9091, 0.9091]  \n",
       "2            [0.96, 0.6957, 0.96, 0.88, 0.8]  \n",
       "1   [0.8333, 0.6364, 0.7826, 0.9091, 0.8696]  \n",
       "6   [0.5455, 0.2222, 0.6957, 0.5714, 0.4762]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'model': model_names,\n",
    "    'mean f1': f1_mean,\n",
    "    'std f1': f1_std,\n",
    "    'all score': all_f1\n",
    "}).sort_values('mean f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict Benchmark Model to Test Set**\n",
    "\n",
    "Untuk mengukur performa akhir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Models\n",
    "models = [logreg_pipeline, knn_pipeline, dtree_pipeline, \n",
    "          voting_clf_hard, voting_clf_soft, stacking_clf,\n",
    "          bagging_clf,rf_clf,ab_clf,gb_clf,xgb_clf]\n",
    "\n",
    "# Create list to store evaluation score\n",
    "list_f1 = []\n",
    "dict_pred = {}\n",
    "dict_proba = {}\n",
    "\n",
    "# Predict to test set\n",
    "for model, name in zip(models, model_names):\n",
    "\n",
    "    # fitting to train set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict to test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    dict_pred[name] = y_pred\n",
    "\n",
    "    if model in [logreg_pipeline, knn_pipeline, dtree_pipeline, voting_clf_soft]:\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        dict_proba[name] = y_proba[:, 1].round(4)\n",
    "\n",
    "    # evaluate score\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    list_f1.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stacking Classifier</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Voting Classifier (Soft)</td>\n",
       "      <td>0.903226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Voting Classifier (Hard)</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeigborsClassifier</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  F1 score\n",
       "9   GradientBoostingClassifier  1.000000\n",
       "5          Stacking Classifier  0.967742\n",
       "8           AdaBoostClassifier  0.967742\n",
       "2                 DecisionTree  0.941176\n",
       "10               XGBClassifier  0.933333\n",
       "4     Voting Classifier (Soft)  0.903226\n",
       "0           LogisticRegression  0.896552\n",
       "3     Voting Classifier (Hard)  0.896552\n",
       "1          KNeigborsClassifier  0.785714\n",
       "7       RandomForestClassifier  0.769231\n",
       "6            BaggingClassifier  0.560000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'model': model_names,\n",
    "    'F1 score': list_f1\n",
    "}).sort_values('F1 score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        71\n",
      "           1       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        87\n",
      "   macro avg       1.00      1.00      1.00        87\n",
      "weighted avg       1.00      1.00      1.00        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "final_model = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=.1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred),annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
